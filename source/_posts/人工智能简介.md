---
title: <font size=7><b>人工智能简介</b></font> 
tags: [人工智能]   
top: 10
categories: 人工智能   
date: 2017-07-28 18:00:00
---

# 备忘
## 三巨头介绍[^1]：Geoffrey Hinton、Yann LeCun、Yoshua Bengio（年级由大到小）
+ Hinton 多伦多大学
DBN
+ LeCun   
CNN
+ Bengio   
attention、GAN

## 神经网络简介
![极简分类](http://res.cloudinary.com/do7yb5qw4/image/upload/v1500626522/人工智能/极简分类_qygbx8.jpg)
<!-- more -->
![极简史](http://res.cloudinary.com/do7yb5qw4/image/upload/v1500626520/人工智能/极简史1_trnlla.jpg)
![极简史2](http://res.cloudinary.com/do7yb5qw4/image/upload/v1500626520/人工智能/极简史2_f7v7jy.jpg)


## 技术备忘
+ 单层线性、双层非线性[^3]    
   >隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。
+ 层数的影响[^3]
   >参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。在最新一届的ImageNet大赛上，目前拿到最好成绩的MSRA团队的方法使用的更是一个深达152层的网络！关于这个方法更多的信息有兴趣的可以查阅ImageNet网站。
+ BP的缺点[^4]
   >基于梯度下降的反向传播法很容易在训练网络参数时收敛于局部极小值。此外，反向传播法训练网络参数还存在很多实际问题，比如需要大量的标签样本来训练网络的权值，多隐含层的神经网络权值的训练速度很慢，权值的修正随着反向传播层数的增加逐渐削弱等。   
   >面对采用反向传播法来训练具有多隐含层的深度网络的网络参数时存在的缺陷，一部分研究人员开始探索通过改变感知器的结构来改善网络学习的性能，由此产生了很多著名的单隐含层的浅层学习模型，如SVM、logistic regression、Maximum entropy model和朴素贝叶斯模型等。浅层学习模型能够有效地解决简单或者具有复杂条件限制的问题，但受限于只含一个隐含层，所以浅层学习模型特征构造的能力有限，不能有效处理包含复杂特征的问题。



## 历史节点
+ 1943 
W.Pitts & W.McCulloch: Concept of ANN(Artificial Neural Network)[^4]
+ 1949
D. Olding Hebb: math model
+ 1957   
F.Rosenblatt: Perceptron   
+ 1969   
Marvin Minsky & Seymour Papert: Perceptron's inability of XOR & limitation of computing power
+ 1974   
Paul Werbos: BP(Back Propagation)
+ 1980   
Hinton: MLP(Multi_Layer Perceptron)
+ 1982
John Hopfield: Hopfield Network(pioneer RNN)
+ 1998
LeCun: CNN(Convoluted Neural Network)
+ 2006   
Hinton, <<Science>> DBN(Deep Belief Networks)
+ 2012[^3]   
   - 深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，   
   - Jeff Dean & Andrew Ng in GoogleBrain: "the Cat"


## 名词概念
+ Grid Search（网格搜索：用尝试的方法选择中间层的节点数）[^3]   
+ generalization[^3]   
提升模型在测试集上的预测效果的主题叫做泛化
+ regularization[^3]   
相关方法被称作正则化。神经网络中常用的泛化技术有权重衰减等。
+ capcity[^3]
更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。
+ DBN(深度信念网络)
+ activation function[^3]
【示意效果】   
![activation function](http://res.cloudinary.com/do7yb5qw4/image/upload/c_scale,w_0.5/v1500626521/人工智能/activation_function_uykbzx.jpg)
+ the curse of dimensionality（维数灾祸）[^5]
   >在机器学习里是指随着样本空间和参数空间维数的增加，潜在参数组合的数量指数型增长，相同精确度的预测会需要多得多的训练样本。当训练样本不变时，预测的精度与空间维数的增加成反比。


# 技术成熟的条件
## Andrew Ng[^2]
   >+ 一个新科技的出现要掐着时间，不能太早也能太晚。比如说iphone，2007年才是它发布的正确时间，而不是在1993年，因为那时的芯片，电池，屏幕技术还没到位。   
+ 另外一个极端的例子是达芬奇发明直升机，他发明直升机的时间是1480年代，而飞机引擎技术在1900年代才出现。   
+ 还有自动驾驶技术，2007年研究自动驾驶技术还太早，因为AI要用到的传感器还没生产出来。2015年以后，整个自动驾驶的生态系统才算比较完善。   
+ 同样在1990年代，网络，视频流还不足以支撑慕课问世，到2011年，整个网络基础建设才为在线视频教育提供了较好的环境。   
+ 深度学习也是，90年代数据/计算比较小，浅层算法效果更好。从2007开始，有了大数据做基础，深度学习才取得了更好的效果。   
+ 但是，我们还是要感谢历史中所有的革新者，包括早期的那些人，他们的工作对后来的发展进步也非常具有影响和帮助。【现在不叫座，以后可能叫好】   

# reference
[^1]: [Yoshua Bengio为什么能跟Hinton、LeCun相提并论？](https://www.zhihu.com/question/37922364) 包含学术传承与贡献的讨论    
[^2]: [【干货|持更】人工智能产业的相关资料及入门指南](https://www.douban.com/note/629481991/) 【已读】   
[^3]: [历史|从神经元到深度学习](https://zhuanlan.zhihu.com/p/23523568) 【可再读】
[^4]: [深度学习技术发展历史及应用现状如何？](https://www.zhihu.com/question/60148469)
[^5]: [神经网络的历史？](https://www.zhihu.com/question/54139566)

## 历史
[a brief history of maching learning(机器学习简史)](http://www.it610.com/article/5198053.htm) 【已读】 

## 可读
[colah blog](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)  
[book: Deep learning(Ian. Goodfellow)](www.deeplearningbook.org/)
[book: Neural networks and deep learning(Michael Nielsen)]
(www.deeplearningbook.org/)
